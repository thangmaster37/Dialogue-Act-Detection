{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10339913,"sourceType":"datasetVersion","datasetId":6402655}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:09.726469Z","iopub.execute_input":"2024-12-31T07:28:09.726721Z","iopub.status.idle":"2024-12-31T07:28:10.090354Z","shell.execute_reply.started":"2024-12-31T07:28:09.726687Z","shell.execute_reply":"2024-12-31T07:28:10.089486Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dialogue-act/scripts/mapping.pair\n/kaggle/input/dialogue-act/data/utterance_dialogue_act.py\n/kaggle/input/dialogue-act/data/raw_data/README.md\n/kaggle/input/dialogue-act/data/raw_data/schema.json\n/kaggle/input/dialogue-act/data/raw_data/dialog_acts.json\n/kaggle/input/dialogue-act/data/raw_data/val/dialogues_002.json\n/kaggle/input/dialogue-act/data/raw_data/val/dialogues_001.json\n/kaggle/input/dialogue-act/data/raw_data/test/dialogues_002.json\n/kaggle/input/dialogue-act/data/raw_data/test/dialogues_001.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_011.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_002.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_007.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_005.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_008.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_017.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_004.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_012.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_010.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_013.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_009.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_014.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_016.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_006.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_001.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_003.json\n/kaggle/input/dialogue-act/data/raw_data/train/dialogues_015.json\n/kaggle/input/dialogue-act/data/processed_data/train.json\n/kaggle/input/dialogue-act/data/processed_data/test.json\n/kaggle/input/dialogue-act/data/processed_data/val.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/dialogue-act/scripts')\nsys.path.append('/kaggle/input/dialogue-act/data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:10.091147Z","iopub.execute_input":"2024-12-31T07:28:10.091487Z","iopub.status.idle":"2024-12-31T07:28:10.095482Z","shell.execute_reply.started":"2024-12-31T07:28:10.091467Z","shell.execute_reply":"2024-12-31T07:28:10.094326Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"<h1>Khai báo thư viện</h1>","metadata":{}},{"cell_type":"code","source":"import json\nfrom transformers import BertTokenizer, BertModel\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import f1_score, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:10.096449Z","iopub.execute_input":"2024-12-31T07:28:10.096730Z","iopub.status.idle":"2024-12-31T07:28:15.561634Z","shell.execute_reply.started":"2024-12-31T07:28:10.096705Z","shell.execute_reply":"2024-12-31T07:28:15.560866Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<h1>Hàm để chuẩn hóa các utterance</h1>","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\n# Đọc các cặp từ viết tắt và từ thay thế từ file 'mapping.pair'\nwith open('/kaggle/input/dialogue-act/scripts/mapping.pair') as fin:\n    replacements = []\n    for line in fin.readlines():\n        tok_from, tok_to = line.replace('\\n', '').split('\\t')\n        replacements.append((' ' + tok_from + ' ', ' ' + tok_to + ' '))  # Đảm bảo từ được bao quanh bởi khoảng trắng\n\n# Hàm thay thế từ viết tắt trong văn bản\ndef replace_abbreviations(text):\n    text = ' ' + text + ' '  # Thêm khoảng trắng đầu và cuối để dễ xử lý\n    for fromx, tox in replacements:\n        text = text.replace(fromx, tox)  # Thay thế tất cả các từ viết tắt\n    return text[1:-1]  # Loại bỏ khoảng trắng thừa ở đầu và cuối\n\n# Hàm làm sạch văn bản (normalize)\ndef normalize(text):\n    # Các bước chuẩn hóa văn bản của bạn (đã được rút gọn và tối giản)\n    text = text.lower()  # Chuyển tất cả chữ thành chữ thường\n    text = re.sub(r'^\\s*|\\s*$', '', text)  # Xóa khoảng trắng ở đầu và cuối\n\n    # Các thay thế cố định khác\n    text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n    text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n\n    # Xử lý số điện thoại và mã bưu điện (theo mẫu có sẵn)\n    text = re.sub(r'(\\d{3})[-.\\s]?(\\d{3})[-.\\s]?(\\d{4,5})', r'\\1\\2\\3', text)  # Lưu số điện thoại không dấu\n\n    # Xử lý thời gian (giả sử chúng ta muốn chuẩn hóa các giá trị thời gian)\n    text = re.sub(r\"\\d{1,2}[:]\\d{2}\", \"[value_time]\", text)\n\n    # Xử lý giá tiền (giả sử các giá trị tiền có định dạng như 100.00 hoặc 100,000.00)\n    text = re.sub(r\"\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", \"[value_price]\", text)\n\n    # Các thay thế khác\n    text = re.sub(r'[^\\w\\s]', '', text)  # Xóa dấu câu không cần thiết\n    text = re.sub(r'\\s+', ' ', text)  # Loại bỏ khoảng trắng dư thừa\n\n    # Thực hiện thay thế từ viết tắt\n    text = replace_abbreviations(text)\n\n    return text\n\n# Hàm token hóa câu\ndef tokenize_text(text):\n    # Tách câu thành các từ bằng cách sử dụng phương pháp token hóa đơn giản (cách này có thể thay thế bằng một thư viện token hóa mạnh mẽ hơn như nltk hoặc spaCy nếu cần)\n    tokens = text.split()\n    return tokens\n\n# Hàm chuẩn hóa văn bản và lấy các token từ câu\ndef process_utterance(text):\n    normalized_text = normalize(text)  # Chuẩn hóa văn bản\n    tokens = tokenize_text(normalized_text)  # Token hóa văn bản đã chuẩn hóa\n    return ' '.join(tokens)\n\n# Hàm chuẩn hóa văn bản và lấy các token từ câu\ndef process_text(text):\n    normalized_text = normalize(text)  # Chuẩn hóa văn bản\n    tokens = tokenize_text(normalized_text)  # Token hóa văn bản đã chuẩn hóa\n    return tokens\n\n# Hàm chính để xử lý từng utterance và trả về dictionary\ndef process_utterances(data):\n\n    processed_data = {}\n    for item in data:\n        utterance = item['utterance']\n        \n        # Làm sạch và thay thế từ viết tắt trong utterance\n        tokens = process_text(utterance)\n\n        # Lưu trữ thông tin sau khi xử lý, sử dụng các token của utterance\n        processed_data[' '.join(tokens)] = item['dialogue_acts']\n    return processed_data\n\n# Hàm kiểm tra có tồn tại ký tự đặc biệt trong văn bản không\ndef contains_special_chars(text):\n    special_chars = string.punctuation\n    return any(char in special_chars for char in text)\n\n# Hàm làm sạch các ký tự đặc biệt\ndef clean_special_chars(text):\n    return ''.join(char for char in text if char not in string.punctuation)\n\n# Hàm chính để kiểm tra các đặc điểm của văn bản\ndef analyze_text(text):\n    # Kiểm tra xem có chứa ký tự đặc biệt hay không\n    if contains_special_chars(text):\n        print(\"Text contains special characters.\")\n    else:\n        print(\"Text does not contain special characters.\")\n\n    # Làm sạch văn bản\n    cleaned_text = clean_special_chars(text)\n    print(f\"Cleaned text: {cleaned_text}\")\n\n\nif __name__ == \"__main__\":\n\n    # Ví dụ dữ liệu\n    data = [\n        {\n            \"utterance\": \"I'm looking for a B&B near the park, do you have any available?\",\n            \"dialogue_acts\": {\n                \"Hotel-Request\": [\n                    [\"location\", \"park\"],\n                    [\"availability\", \"yes\"]\n                ]\n            }\n        },\n        {\n            \"utterance\": \"Can you tell me the price for a room for two?\",\n            \"dialogue_acts\": {\n                \"Hotel-Request\": [\n                    [\"time\", \"two\"]\n                ]\n            }\n        }\n    ]\n\n    # Xử lý dữ liệu\n    processed_utterances = process_utterances(data)\n\n    # In ra kết quả\n    print(\"Processed Utterances:\")\n    for utterance, dialogue_acts in processed_utterances.items():\n        print(f\"Utterance: {utterance}\")\n        print(f\"Dialogue Acts: {dialogue_acts}\")\n\n    # Kiểm tra các đặc điểm của một câu\n    example_text = \"I'm looking for a B&B near the park!\"\n    analyze_text(example_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.563365Z","iopub.execute_input":"2024-12-31T07:28:15.563836Z","iopub.status.idle":"2024-12-31T07:28:15.581633Z","shell.execute_reply.started":"2024-12-31T07:28:15.563811Z","shell.execute_reply":"2024-12-31T07:28:15.580717Z"}},"outputs":[{"name":"stdout","text":"Processed Utterances:\nUtterance: im looking for a bed and breakfast near the park do you have any available\nDialogue Acts: {'Hotel-Request': [['location', 'park'], ['availability', 'yes']]}\nUtterance: can you tell me the price for a room for 2\nDialogue Acts: {'Hotel-Request': [['time', 'two']]}\nText contains special characters.\nCleaned text: Im looking for a BB near the park\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<h1>Hàm để load data gồm: utterance và dialogue act</h1>","metadata":{}},{"cell_type":"code","source":"# Function to load data from JSON files\ndef load_data_from_json(file_path):\n    \"\"\"\n    Loads the dataset from a JSON file where each utterance is associated with dialogue acts and their attributes.\n    \n    Args:\n        file_path (str): Path to the JSON file.\n    \n    Returns:\n        List of tuples: Each tuple contains an utterance and its corresponding dialogue acts.\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    # Extract utterances and dialogue acts\n    utterance_dialogue_act_dict = {}\n    for item in data:\n        utterance = process_utterance(item['utterance'])\n        dialogue_acts = item['dialogue_acts']\n        # Convert dialogue acts into a list of labels (as an example of how you can format them)\n        labels = []\n        for act, attributes in dialogue_acts.items():\n            for attribute in attributes:\n                # Create a unique label for each dialogue act and its attributes (e.g., Train-Inform-departure)\n                labels.append(f\"{act}-{attribute[0]}\")\n        utterance_dialogue_act_dict[utterance] = labels\n\n    return utterance_dialogue_act_dict\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.582822Z","iopub.execute_input":"2024-12-31T07:28:15.583128Z","iopub.status.idle":"2024-12-31T07:28:15.590000Z","shell.execute_reply.started":"2024-12-31T07:28:15.583087Z","shell.execute_reply":"2024-12-31T07:28:15.589386Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<h1>Khởi tạo các class:</h1>\n<h3>+ DialogueActDataset</h3>\n<h3>+ BaseClassifier</h3>\n<h3>+ BERTClassifier</h3>","metadata":{}},{"cell_type":"code","source":"# Dataset class for token classification\nclass DialogueActDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for Dialogue Act data.\n    \"\"\"\n    def __init__(self, data, tokenizer, max_len, label_list):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.label_list = label_list\n        self.num_classes = len(label_list)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        utterance, labels = self.data[idx]\n        tokens = self.tokenizer(utterance, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n        input_ids = tokens['input_ids'].squeeze(0)\n        attention_mask = tokens['attention_mask'].squeeze(0)\n        \n        # Create a binary vector for labels with 1's at positions of the relevant acts\n        label_ids = torch.zeros(self.num_classes)\n        for label in labels:\n            if label in self.label_list:\n                label_index = self.label_list.index(label)\n                label_ids[label_index] = 1  # Set 1 for the relevant label\n        \n        return input_ids, attention_mask, label_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.590710Z","iopub.execute_input":"2024-12-31T07:28:15.591009Z","iopub.status.idle":"2024-12-31T07:28:15.607746Z","shell.execute_reply.started":"2024-12-31T07:28:15.590989Z","shell.execute_reply":"2024-12-31T07:28:15.606891Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Base model class for sequence labelling\nclass BaseClassifier(nn.Module):\n    \"\"\"\n    Base class for sequence classification models.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, num_labels):\n        super(BaseClassifier, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_labels = num_labels\n        self.classifier = nn.Linear(hidden_dim, num_labels)\n\n    def forward(self, x):\n        logits = self.classifier(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.608575Z","iopub.execute_input":"2024-12-31T07:28:15.609059Z","iopub.status.idle":"2024-12-31T07:28:15.635822Z","shell.execute_reply.started":"2024-12-31T07:28:15.609028Z","shell.execute_reply":"2024-12-31T07:28:15.635124Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# BERT-based classifier\nclass BERTClassifier(BaseClassifier):\n    def __init__(self, encoder_name='bert-base-uncased', num_labels=10):\n        bert_model = BertModel.from_pretrained(encoder_name)\n        super().__init__(input_dim=bert_model.config.hidden_size, hidden_dim=bert_model.config.hidden_size, num_labels=num_labels)\n        self.bert = bert_model\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        return super().forward(outputs.last_hidden_state[:, 0, :])  # Use the first token (CLS token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.636594Z","iopub.execute_input":"2024-12-31T07:28:15.636821Z","iopub.status.idle":"2024-12-31T07:28:15.649545Z","shell.execute_reply.started":"2024-12-31T07:28:15.636792Z","shell.execute_reply":"2024-12-31T07:28:15.648837Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"<h1>Huấn luyện mô hình và đánh giá mô hình</h1>","metadata":{}},{"cell_type":"code","source":"# Training and evaluation function\ndef train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs=20, device='cpu'):\n    model = model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for input_ids, attention_mask, labels in train_loader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(input_ids, attention_mask)  # Chuyển cả input_ids và attention_mask vào mô hình\n            loss = criterion(logits, labels)  # BCEWithLogitsLoss cho multi-label classification\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader):.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for input_ids, attention_mask, labels in val_loader:\n                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n                logits = model(input_ids, attention_mask)  # Chuyển cả input_ids và attention_mask vào mô hình\n                loss = criterion(logits, labels)\n                val_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader):.4f}\")\n\n    # Evaluation trên test data\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in test_loader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            preds = torch.sigmoid(model(input_ids, attention_mask))  # Chuyển cả input_ids và attention_mask vào mô hình\n            all_preds.append(preds)\n            all_labels.append(labels)\n\n    # Flatten predictions và labels\n    all_preds = torch.cat(all_preds, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n\n    # Threshold predictions (ví dụ: nếu sigmoid output > 0.5, coi là 1)\n    all_preds = (all_preds > 0.5).float()\n\n    # Tính toán F1-score cho từng lớp\n    f1 = f1_score(all_labels.cpu(), all_preds.cpu(), average='macro')  # Tính F1-score với average là 'macro'\n    print(f\"F1 Score (Macro): {f1:.4f}\")\n\n    # Cũng có thể sử dụng classification_report để hiển thị các chỉ số như precision, recall và F1 cho mỗi lớp\n    report = classification_report(all_labels.cpu(), all_preds.cpu(), target_names=[f\"Class {i}\" for i in range(all_labels.size(1))])\n    print(\"Classification Report:\")\n    print(report)\n\n    # Evaluation metric (ví dụ: accuracy)\n    accuracy = (all_preds == all_labels).float().mean()\n    print(f\"Accuracy on test set: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.650277Z","iopub.execute_input":"2024-12-31T07:28:15.650557Z","iopub.status.idle":"2024-12-31T07:28:15.665952Z","shell.execute_reply.started":"2024-12-31T07:28:15.650529Z","shell.execute_reply":"2024-12-31T07:28:15.665319Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Predict function to test with any utterance\ndef predict(model, utterance, tokenizer, label_list, max_len=50, device='cpu'):\n    model.eval()  # Chuyển sang chế độ đánh giá\n    tokens = tokenizer(utterance, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n    input_ids = tokens['input_ids'].squeeze(0).to(device)\n    attention_mask = tokens['attention_mask'].squeeze(0).to(device)\n\n    with torch.no_grad():\n        logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n\n    preds = torch.sigmoid(logits).cpu().numpy()\n    preds = (preds > 0.5).astype(int)  # Chuyển kết quả thành 0 hoặc 1\n\n    predicted_labels = [label_list[i] for i in range(len(preds[0])) if preds[0][i] == 1]\n    return predicted_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.666572Z","iopub.execute_input":"2024-12-31T07:28:15.666756Z","iopub.status.idle":"2024-12-31T07:28:15.677685Z","shell.execute_reply.started":"2024-12-31T07:28:15.666740Z","shell.execute_reply":"2024-12-31T07:28:15.676975Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Main function\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load the train, validation, and test data from JSON files\n    train_data = load_data_from_json('/kaggle/input/dialogue-act/data/processed_data/train.json')\n    val_data = load_data_from_json('/kaggle/input/dialogue-act/data/processed_data/val.json')\n    test_data = load_data_from_json('/kaggle/input/dialogue-act/data/processed_data/test.json')\n\n    label_list = list(set([label for labels in train_data.values() for label in labels]))\n\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    max_len = 50\n\n    train_dataset = DialogueActDataset(list(train_data.items()), tokenizer, max_len, label_list)\n    val_dataset = DialogueActDataset(list(val_data.items()), tokenizer, max_len, label_list)\n    test_dataset = DialogueActDataset(list(test_data.items()), tokenizer, max_len, label_list)\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n\n    bert_model = BERTClassifier(num_labels=len(label_list)).to(device)\n    optimizer = optim.Adam(bert_model.parameters(), lr=1e-5)\n    criterion = nn.BCEWithLogitsLoss()\n\n    print(\"Training BERT Model\")\n    train_and_evaluate_model(bert_model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs=10, device=device)\n\n    # Save the model\n    torch.save(bert_model.state_dict(), '/kaggle/working/bert_model.pth')\n\n    # Test with a new utterance\n    utterance = \"What type of food would you like?\"\n    predicted_labels = predict(bert_model, utterance, tokenizer, label_list, max_len, device)\n    print(f\"Predicted Dialogue Acts for the utterance: {predicted_labels}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.678533Z","iopub.execute_input":"2024-12-31T07:28:15.678750Z","iopub.status.idle":"2024-12-31T07:28:15.695291Z","shell.execute_reply.started":"2024-12-31T07:28:15.678733Z","shell.execute_reply":"2024-12-31T07:28:15.694633Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:28:15.695894Z","iopub.execute_input":"2024-12-31T07:28:15.696115Z","iopub.status.idle":"2024-12-31T09:17:41.991478Z","shell.execute_reply.started":"2024-12-31T07:28:15.696097Z","shell.execute_reply":"2024-12-31T09:17:41.990550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2a62a45d154f6883a0e8bd53d211e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefd88c1e8214724a6ac5474f42404d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802bdfaecc7841fea509e211e78cf223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eca1d86e8a84b9d92c76151d5237ce9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e2984686dc4820b35f46e5faf511e2"}},"metadata":{}},{"name":"stdout","text":"Training BERT Model\nEpoch 1/10, Training Loss: 0.0521\nEpoch 1/10, Validation Loss: 0.0138\nEpoch 2/10, Training Loss: 0.0113\nEpoch 2/10, Validation Loss: 0.0082\nEpoch 3/10, Training Loss: 0.0081\nEpoch 3/10, Validation Loss: 0.0071\nEpoch 4/10, Training Loss: 0.0068\nEpoch 4/10, Validation Loss: 0.0065\nEpoch 5/10, Training Loss: 0.0061\nEpoch 5/10, Validation Loss: 0.0063\nEpoch 6/10, Training Loss: 0.0055\nEpoch 6/10, Validation Loss: 0.0063\nEpoch 7/10, Training Loss: 0.0050\nEpoch 7/10, Validation Loss: 0.0062\nEpoch 8/10, Training Loss: 0.0046\nEpoch 8/10, Validation Loss: 0.0064\nEpoch 9/10, Training Loss: 0.0042\nEpoch 9/10, Validation Loss: 0.0066\nEpoch 10/10, Training Loss: 0.0038\nEpoch 10/10, Validation Loss: 0.0067\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"},{"name":"stdout","text":"F1 Score (Macro): 0.5040\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.90      0.56      0.69        16\n     Class 1       0.90      0.81      0.85        47\n     Class 2       0.94      0.94      0.94       622\n     Class 3       1.00      0.11      0.20        18\n     Class 4       0.87      0.91      0.89       315\n     Class 5       0.81      0.83      0.82       150\n     Class 6       0.68      0.70      0.69       143\n     Class 7       0.00      0.00      0.00         0\n     Class 8       0.75      0.76      0.75        50\n     Class 9       0.62      0.77      0.69       336\n    Class 10       0.00      0.00      0.00         2\n    Class 11       0.89      0.93      0.91        71\n    Class 12       0.82      0.82      0.82       409\n    Class 13       0.00      0.00      0.00         1\n    Class 14       0.86      0.85      0.85       478\n    Class 15       0.91      0.97      0.94       885\n    Class 16       0.78      0.39      0.52        18\n    Class 17       0.00      0.00      0.00         0\n    Class 18       0.96      0.96      0.96       305\n    Class 19       0.00      0.00      0.00         1\n    Class 20       0.85      0.75      0.80       130\n    Class 21       0.45      0.36      0.40        28\n    Class 22       0.85      0.83      0.84       557\n    Class 23       0.00      0.00      0.00         0\n    Class 24       0.85      0.88      0.86       239\n    Class 25       0.00      0.00      0.00         0\n    Class 26       0.42      0.78      0.55        32\n    Class 27       0.00      0.00      0.00         0\n    Class 28       0.00      0.00      0.00         0\n    Class 29       1.00      0.56      0.71         9\n    Class 30       0.88      0.93      0.90        40\n    Class 31       0.00      0.00      0.00         1\n    Class 32       0.00      0.00      0.00         1\n    Class 33       0.47      0.64      0.55        14\n    Class 34       0.00      0.00      0.00         0\n    Class 35       0.00      0.00      0.00         0\n    Class 36       0.63      0.77      0.69       140\n    Class 37       0.95      0.93      0.94       195\n    Class 38       0.90      0.84      0.87       124\n    Class 39       0.88      0.63      0.73        78\n    Class 40       0.00      0.00      0.00         0\n    Class 41       0.82      0.84      0.83        38\n    Class 42       0.00      0.00      0.00         0\n    Class 43       0.00      0.00      0.00         1\n    Class 44       0.86      0.88      0.87       336\n    Class 45       0.00      0.00      0.00         1\n    Class 46       0.84      0.85      0.84       455\n    Class 47       0.00      0.00      0.00         0\n    Class 48       0.72      0.90      0.80        20\n    Class 49       0.98      0.97      0.97        98\n    Class 50       0.84      0.96      0.90        72\n    Class 51       0.70      0.39      0.50        18\n    Class 52       0.00      0.00      0.00         0\n    Class 53       0.00      0.00      0.00         0\n    Class 54       0.98      0.94      0.96       116\n    Class 55       0.89      0.96      0.93       112\n    Class 56       1.00      0.12      0.22         8\n    Class 57       0.52      0.61      0.56        28\n    Class 58       0.33      0.14      0.20        29\n    Class 59       0.93      0.88      0.90       267\n    Class 60       0.92      0.84      0.88       227\n    Class 61       0.95      0.94      0.95       658\n    Class 62       0.60      0.75      0.67         4\n    Class 63       0.00      0.00      0.00         0\n    Class 64       0.79      0.76      0.78        75\n    Class 65       0.64      0.68      0.66        34\n    Class 66       0.47      0.64      0.55        14\n    Class 67       0.91      0.68      0.78        31\n    Class 68       0.00      0.00      0.00         0\n    Class 69       0.94      0.87      0.90       239\n    Class 70       0.93      0.78      0.85       193\n    Class 71       0.76      0.43      0.55        30\n    Class 72       0.00      0.00      0.00         0\n    Class 73       0.70      0.46      0.55        35\n    Class 74       0.94      0.94      0.94        68\n    Class 75       0.00      0.00      0.00         0\n    Class 76       0.77      0.71      0.74        14\n    Class 77       0.85      0.91      0.88       321\n    Class 78       0.86      0.88      0.87       591\n    Class 79       0.33      1.00      0.50         1\n    Class 80       0.94      0.94      0.94        16\n    Class 81       0.67      0.36      0.47        11\n    Class 82       1.00      0.67      0.80         3\n    Class 83       0.00      0.00      0.00         0\n    Class 84       0.85      0.93      0.89       534\n    Class 85       0.94      0.92      0.93       190\n    Class 86       0.00      0.00      0.00         0\n    Class 87       0.37      0.21      0.26        34\n    Class 88       0.86      0.93      0.89       228\n    Class 89       0.00      0.00      0.00         3\n    Class 90       0.00      0.00      0.00         0\n    Class 91       0.66      0.71      0.69        49\n    Class 92       0.77      0.82      0.79        44\n    Class 93       0.00      0.00      0.00         0\n    Class 94       0.00      0.00      0.00         1\n    Class 95       0.00      0.00      0.00         1\n    Class 96       0.80      0.77      0.78       103\n    Class 97       0.77      0.57      0.65        30\n    Class 98       0.00      0.00      0.00         4\n    Class 99       0.83      0.74      0.78        39\n   Class 100       0.00      0.00      0.00         0\n   Class 101       0.61      0.45      0.52        42\n   Class 102       0.90      0.98      0.93        81\n   Class 103       0.00      0.00      0.00         5\n   Class 104       0.00      0.00      0.00         3\n   Class 105       0.85      0.57      0.69        61\n   Class 106       0.81      0.84      0.82       158\n   Class 107       0.88      0.82      0.85       100\n   Class 108       0.00      0.00      0.00         0\n   Class 109       0.00      0.00      0.00         0\n   Class 110       0.29      0.29      0.29        14\n   Class 111       0.90      0.90      0.90       184\n   Class 112       0.00      0.00      0.00         5\n   Class 113       0.00      0.00      0.00         0\n   Class 114       0.79      0.77      0.78       153\n   Class 115       0.92      0.91      0.92       265\n   Class 116       0.00      0.00      0.00         0\n   Class 117       0.96      0.95      0.96       715\n   Class 118       0.00      0.00      0.00         5\n   Class 119       0.93      0.94      0.94       347\n   Class 120       0.88      0.93      0.90       286\n   Class 121       0.96      0.98      0.97       181\n   Class 122       0.87      0.93      0.90        14\n   Class 123       0.00      0.00      0.00         1\n   Class 124       0.00      0.00      0.00         0\n   Class 125       0.92      0.84      0.88       274\n   Class 126       0.95      0.71      0.81       127\n   Class 127       0.93      0.84      0.88       343\n   Class 128       0.55      0.75      0.63         8\n   Class 129       0.67      1.00      0.80         2\n   Class 130       0.67      0.44      0.53         9\n   Class 131       0.95      0.94      0.95       119\n   Class 132       0.96      0.94      0.95       103\n   Class 133       0.70      0.70      0.70        47\n   Class 134       0.00      0.00      0.00         5\n   Class 135       0.95      0.91      0.93        45\n   Class 136       0.81      1.00      0.90        13\n   Class 137       0.54      0.51      0.52        53\n   Class 138       0.00      0.00      0.00         0\n   Class 139       0.00      0.00      0.00         1\n   Class 140       0.00      0.00      0.00        16\n   Class 141       0.00      0.00      0.00         3\n   Class 142       0.82      0.74      0.78        19\n   Class 143       0.94      0.80      0.86        93\n   Class 144       0.00      0.00      0.00         4\n   Class 145       0.77      0.75      0.76       117\n   Class 146       0.50      0.25      0.33         4\n   Class 147       0.00      0.00      0.00         0\n   Class 148       0.91      0.88      0.89       120\n   Class 149       0.87      0.89      0.88      1384\n   Class 150       0.50      0.18      0.26        17\n   Class 151       0.45      0.42      0.43        12\n   Class 152       0.00      0.00      0.00         0\n   Class 153       0.66      0.54      0.59        84\n   Class 154       0.00      0.00      0.00         0\n   Class 155       0.57      0.54      0.55        56\n   Class 156       0.77      0.55      0.64        73\n   Class 157       0.25      0.33      0.29         3\n   Class 158       0.67      0.75      0.71         8\n   Class 159       0.40      0.67      0.50         3\n   Class 160       0.74      0.67      0.70       135\n   Class 161       1.00      0.33      0.50         3\n   Class 162       1.00      1.00      1.00         1\n   Class 163       0.75      0.92      0.83        39\n   Class 164       0.89      0.98      0.93       508\n   Class 165       0.67      0.22      0.33         9\n   Class 166       0.46      0.75      0.57         8\n   Class 167       1.00      1.00      1.00         1\n   Class 168       0.00      0.00      0.00         1\n   Class 169       0.85      0.83      0.84       133\n   Class 170       0.93      0.65      0.76        20\n   Class 171       0.94      0.92      0.93       676\n   Class 172       0.88      0.89      0.89        93\n   Class 173       0.84      0.93      0.88       315\n   Class 174       0.55      0.51      0.53       100\n   Class 175       1.00      0.08      0.15        12\n   Class 176       1.00      1.00      1.00         2\n   Class 177       0.92      0.94      0.93       187\n   Class 178       0.81      0.95      0.87        40\n   Class 179       0.00      0.00      0.00         3\n   Class 180       0.78      0.67      0.72        54\n   Class 181       0.00      0.00      0.00         1\n   Class 182       0.00      0.00      0.00         0\n   Class 183       0.00      0.00      0.00         3\n   Class 184       0.00      0.00      0.00         0\n   Class 185       0.94      0.99      0.96        88\n   Class 186       0.93      0.95      0.94       103\n   Class 187       0.65      0.53      0.58        76\n   Class 188       0.00      0.00      0.00         0\n   Class 189       0.92      0.95      0.94       350\n   Class 190       0.97      0.95      0.96       149\n   Class 191       0.00      0.00      0.00         0\n   Class 192       0.00      0.00      0.00         0\n   Class 193       0.80      0.71      0.75        99\n   Class 194       0.80      0.87      0.84       108\n   Class 195       0.68      0.68      0.68        40\n   Class 196       0.89      0.91      0.90        64\n   Class 197       0.00      0.00      0.00         5\n   Class 198       0.60      0.50      0.55         6\n   Class 199       0.92      0.94      0.93       141\n   Class 200       0.50      0.52      0.51        21\n   Class 201       0.79      0.75      0.77       565\n   Class 202       0.88      0.90      0.89       102\n   Class 203       0.83      0.96      0.89       155\n   Class 204       0.79      0.89      0.84        47\n   Class 205       0.00      0.00      0.00         6\n   Class 206       0.91      0.78      0.84        88\n   Class 207       0.50      0.20      0.29         5\n   Class 208       0.00      0.00      0.00         3\n   Class 209       0.00      0.00      0.00         0\n   Class 210       0.65      0.68      0.67        50\n   Class 211       0.48      0.27      0.34        41\n   Class 212       0.56      0.42      0.48        12\n   Class 213       0.00      0.00      0.00         3\n   Class 214       0.00      0.00      0.00         0\n   Class 215       0.92      0.95      0.94       713\n   Class 216       0.98      0.93      0.96       148\n   Class 217       0.87      0.85      0.86       455\n   Class 218       0.43      0.71      0.54        28\n   Class 219       0.00      0.00      0.00         0\n   Class 220       0.44      0.37      0.40        30\n   Class 221       0.00      0.00      0.00         0\n   Class 222       0.86      0.87      0.87       546\n   Class 223       0.00      0.00      0.00         2\n   Class 224       0.92      0.90      0.91       410\n   Class 225       0.91      0.95      0.93       280\n   Class 226       1.00      0.21      0.35        14\n   Class 227       0.00      0.00      0.00         3\n   Class 228       0.00      0.00      0.00         0\n   Class 229       0.84      0.83      0.84       453\n   Class 230       0.75      0.60      0.67        25\n   Class 231       0.00      0.00      0.00         0\n   Class 232       0.62      0.29      0.39        28\n   Class 233       0.49      0.70      0.58        37\n   Class 234       0.73      0.44      0.55        50\n   Class 235       0.00      0.00      0.00         3\n   Class 236       1.00      1.00      1.00         1\n   Class 237       1.00      0.20      0.33         5\n   Class 238       0.50      0.57      0.53         7\n   Class 239       0.99      1.00      0.99       202\n   Class 240       0.78      0.46      0.58       171\n   Class 241       0.81      0.77      0.79       124\n   Class 242       0.00      0.00      0.00         0\n   Class 243       0.00      0.00      0.00         0\n   Class 244       0.39      0.33      0.36        21\n   Class 245       0.00      0.00      0.00         2\n   Class 246       0.00      0.00      0.00         0\n   Class 247       0.00      0.00      0.00         3\n   Class 248       0.85      0.68      0.75       204\n   Class 249       0.00      0.00      0.00         0\n   Class 250       0.93      0.93      0.93       520\n   Class 251       0.87      0.95      0.91        42\n   Class 252       0.00      0.00      0.00         5\n   Class 253       0.80      0.36      0.50        11\n   Class 254       0.73      0.60      0.66       137\n   Class 255       0.94      0.66      0.77        70\n   Class 256       0.78      0.88      0.82         8\n   Class 257       0.00      0.00      0.00         1\n   Class 258       1.00      0.33      0.50         6\n   Class 259       0.70      0.93      0.80        92\n   Class 260       1.00      0.33      0.50         3\n   Class 261       0.37      0.35      0.36        20\n   Class 262       0.00      0.00      0.00         0\n   Class 263       0.93      0.94      0.94       687\n   Class 264       0.00      0.00      0.00         0\n   Class 265       0.87      0.89      0.88       275\n   Class 266       0.00      0.00      0.00         1\n   Class 267       0.90      0.88      0.89       254\n   Class 268       0.57      0.31      0.40        13\n   Class 269       0.83      0.52      0.64        73\n   Class 270       0.00      0.00      0.00         1\n   Class 271       0.95      0.87      0.91       640\n   Class 272       0.95      0.94      0.94       187\n   Class 273       0.67      0.44      0.53         9\n   Class 274       0.00      0.00      0.00         2\n   Class 275       0.32      0.04      0.06       222\n   Class 276       0.95      0.95      0.95        44\n   Class 277       0.81      0.58      0.68        38\n   Class 278       1.00      0.98      0.99       200\n   Class 279       0.68      0.76      0.72        17\n   Class 280       0.77      0.75      0.76       110\n   Class 281       0.00      0.00      0.00         4\n\n   micro avg       0.87      0.85      0.86     28992\n   macro avg       0.54      0.50      0.50     28992\nweighted avg       0.86      0.85      0.85     28992\n samples avg       0.83      0.83      0.82     28992\n\nAccuracy on test set: 0.9979\nPredicted Dialogue Acts for the utterance: ['Restaurant-Request-food']\n","output_type":"stream"}],"execution_count":12}]}